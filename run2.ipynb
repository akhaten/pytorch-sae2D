{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Input Libraries'''\n",
    "# import os\n",
    "# import torch\n",
    "# import sklearn\n",
    "# import numpy as np\n",
    "# import torch.nn.functional as F\n",
    "# import torch.utils.data as data_utils\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from functions import mrf as mrf\n",
    "# from functions import models as m\n",
    "# from functions import dataset as data\n",
    "# from functions import training_tools as tt\n",
    "# from functions.visualization import argmax_ch\n",
    "# from functions.parser import train_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# # matplotlib.use('agg')\n",
    "# import matplotlib.pyplot as plt\n",
    "# from functions import visualization as vis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sae.functions.dataset\n",
    "vols_path = './sae/data/vols/'\n",
    "aseg_path = './sae/data/labels/'\n",
    "train_set = sae.functions.dataset.load_bucker_data(vols_path,aseg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mri : torch.Size([1, 1, 160, 192, 224])\n",
      "aseg : torch.Size([160, 192, 224])\n",
      "onehot : torch.Size([1, 14, 160, 192, 224])\n"
     ]
    }
   ],
   "source": [
    "mri, aseg, onehot, _  = train_set[0] \n",
    "print('mri :', mri.shape)\n",
    "print('aseg :', aseg.shape)\n",
    "print('onehot :', onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot[0, :, 70, 70, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aseg[70, 70, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 16\n",
    "\n",
    "# figure = matplotlib.pyplot.figure()\n",
    "# matplotlib.pyplot.subplot(1, 2, 1)\n",
    "# _ = matplotlib.pyplot.imshow(mri[0, 0, index])\n",
    "# matplotlib.pyplot.subplot(1, 2, 2)\n",
    "# _ = matplotlib.pyplot.imshow(aseg[index])\n",
    "# figure.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_path = './sae/data/prob_atlas.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atlas = torch.from_numpy(np.load((atlas_path))['vol_data']).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atlas.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atlas[70, 70, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 160, 192, 224])\n",
      "14 160 192 224\n"
     ]
    }
   ],
   "source": [
    "import sae.functions.dataset\n",
    "template = sae.functions.dataset.get_prob_atlas(atlas_path)\n",
    "print(template.shape)\n",
    "chs = template.shape[1]\n",
    "dim1 = template.shape[2]\n",
    "dim2 = template.shape[3]\n",
    "dim3 = template.shape[4]\n",
    "print(chs, dim1, dim2, dim3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "argm_ch = sae.functions.visualization.argmax_ch(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argm_ch[0, :, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template[0, :, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template[0, :, 70, 70, 70]\n",
    "import sys\n",
    "sys.path.append('./sae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional\n",
    "import torch.optim\n",
    "\n",
    "import numpy\n",
    "\n",
    "import sae.functions.mrf\n",
    "import sae.functions.visualization\n",
    "import sae.functions.training_tools\n",
    "\n",
    "import wrapper.mrf\n",
    "\n",
    "\n",
    "\n",
    "class SAELoss:\n",
    "\n",
    "    \"\"\"\n",
    "    Warnings:\n",
    "        - Running var must be clear to each epoch's end with clear_running_var()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        sigma: float,\n",
    "        prior: torch.Tensor,\n",
    "        alpha: float = 1.0, \n",
    "        beta: float = 0.01, \n",
    "        eps: float = 1e-12,\n",
    "        k: int = 3,\n",
    "        var: float = 1e8\n",
    "    ) -> None:\n",
    "        \n",
    "        self.prior = prior\n",
    "        self.log_prior = torch.log(\n",
    "            sae.functions.training_tools.normalize_dim1(prior+eps)\n",
    "        ).detach()\n",
    "        print('log_prior:', prior.size())\n",
    "        # log_prior: torch.Size([1, 14, 160, 192, 224])\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.sigma = sigma\n",
    "        self.eps = eps\n",
    "        self.k = k\n",
    "\n",
    "        self.var = var\n",
    "\n",
    "        self.lookup = None\n",
    "        if self.beta != 0:\n",
    "            argm_ch = sae.functions.visualization.argmax_ch(self.prior)\n",
    "            argm_ch = argm_ch.type(torch.uint8)\n",
    "            # argm_ch : torch.Size([1, 14, 160, 192, 224])\n",
    "            self.lookup = sae.functions.mrf.get_lookup(\n",
    "                prior = argm_ch,\n",
    "                neighboor_size = self.k\n",
    "            )\n",
    "\n",
    "            print('argm_ch :', argm_ch.size())\n",
    "            print('lookup :', self.lookup)\n",
    "\n",
    "        self.running_var = []\n",
    "\n",
    "\n",
    "    def __call__(self,\n",
    "        x: torch.Tensor,\n",
    "        logits: torch.Tensor,\n",
    "        recon: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        prior_loss = self.compute_prior_loss(logits)\n",
    "        recon_loss = self.compute_recon_loss(x, recon)\n",
    "        consistent = self.compute_consistent(logits)\n",
    "        return prior_loss + recon_loss + consistent\n",
    "\n",
    "    def compute_prior_loss(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        log_pi = torch.nn.functional.log_softmax(logits, 1)\n",
    "        pi = torch.exp(log_pi)\n",
    "        \n",
    "        cce = -1*torch.sum(pi*self.log_prior,1)      #cross entropy\n",
    "        cce = torch.sum(cce,(1,2,3))            #cce over all the dims\n",
    "        cce = cce.mean()               \n",
    "            \n",
    "        h = -1*torch.sum(pi*log_pi,1)\n",
    "        h = torch.sum(h,(1,2,3))\n",
    "        h = h.mean()\n",
    " \n",
    "        prior_loss = cce - h\n",
    "\n",
    "        return prior_loss\n",
    "    \n",
    "    def compute_consistent(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        log_pi = torch.nn.functional.log_softmax(logits, 1)\n",
    "        pi = torch.exp(log_pi)\n",
    "        \n",
    "        if self.beta != 0: # ie not(self.lookup is None)\n",
    "            consistent = self.beta*wrapper.mrf.spatial_consistency(\n",
    "                input = pi,\n",
    "                table = self.lookup,\n",
    "                neighboor_size = self.k\n",
    "            )\n",
    "        else:\n",
    "            consistent = torch.zeros(1, device=logits.device)\n",
    "        \n",
    "        return consistent\n",
    "    \n",
    "    def compute_recon_loss(self, \n",
    "        x: torch.Tensor, \n",
    "        recon: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        if self.sigma == 0:\n",
    "            mse = (recon-x.detach())**2  #mse\n",
    "            mse = torch.sum(mse,(1,2,3,4))    #mse over all dims\n",
    "            mse = mse.mean()                  #avarage over all batches\n",
    "            recon_loss = self.alpha * mse \n",
    "        elif self.sigma == 2:\n",
    "            mse = (recon-x.detach())**2\n",
    "            rounded_var = 10**numpy.round(numpy.log10(self.var))\n",
    "\n",
    "            # Weight Reconstruction loss\n",
    "            mse = numpy.clip(0.5*(1/(rounded_var)),0, 500) * mse\n",
    "            mse = torch.sum(mse,(1,2,3,4))    #mse over all dims\n",
    "            mse = mse.mean()                  #avarage over all batches\n",
    "\n",
    "            self.running_var.append(mse.detach().mean().item())\n",
    "\n",
    "            # Since args.var is a scalar now, we need to account for\n",
    "            # the fact that we doing log det of a matrix\n",
    "            # Therefore, we multiply by the dimension of the image\n",
    "\n",
    "            c = dim1*dim2*dim3 #chs is 1 for image\n",
    "\n",
    "            _var = torch.from_numpy(numpy.array(self.var+self.eps)).float()\n",
    "            recon_loss = mse + 0.5 * c * torch.log(_var)\n",
    "        else:\n",
    "            raise AssertionError('sigma must be 0 or 2')\n",
    "        \n",
    "        return recon_loss\n",
    "    \n",
    "    def normalize_dim1(self, x):\n",
    "        '''\n",
    "        Ensure that dim1 sums up to one for proper probabilistic interpretation\n",
    "        '''\n",
    "        normalizer = torch.sum(x, dim=1, keepdim=True)\n",
    "        return x/normalizer\n",
    "    \n",
    "    def update_variance(self) -> None:\n",
    "        self.var = numpy.mean(self.running_var)\n",
    "\n",
    "    def clear_running_var(self) -> None:\n",
    "        \"\"\" Running var must be clear to each end epoch\n",
    "        \"\"\"\n",
    "        self.running_var.clear()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_prior: torch.Size([1, 14, 160, 192, 224])\n",
      "argm_ch : torch.Size([1, 14, 160, 192, 224])\n",
      "lookup : [[9.93031455e-01 0.00000000e+00 1.36885061e-07 0.00000000e+00\n",
      "  1.54816243e-03 5.32330791e-08 9.12567071e-08 0.00000000e+00\n",
      "  1.36124588e-06 3.38904596e-04 0.00000000e+00 1.52094512e-08\n",
      "  2.40081187e-05 5.05581169e-03]\n",
      " [0.00000000e+00 8.29217037e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.20877635e-02\n",
      "  1.08695200e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.86254424e-04 0.00000000e+00 8.33550630e-01 0.00000000e+00\n",
      "  4.40905610e-02 3.53262557e-02 0.00000000e+00 0.00000000e+00\n",
      "  4.16278637e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 4.52184350e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 8.32721867e-01\n",
      "  0.00000000e+00 0.00000000e+00 1.03891362e-03 1.36927945e-03\n",
      "  8.17003408e-02 0.00000000e+00 7.29456461e-02 0.00000000e+00\n",
      "  0.00000000e+00 1.02239533e-02]\n",
      " [1.05162970e-02 0.00000000e+00 2.20110825e-04 0.00000000e+00\n",
      "  8.62416735e-01 1.33120300e-04 0.00000000e+00 1.59930325e-04\n",
      "  7.46256282e-02 7.61218756e-04 5.41366216e-05 0.00000000e+00\n",
      "  0.00000000e+00 5.11128227e-02]\n",
      " [2.83520187e-05 0.00000000e+00 1.38276845e-02 0.00000000e+00\n",
      "  1.04375932e-02 8.31337891e-01 5.07501134e-03 0.00000000e+00\n",
      "  7.50437431e-02 0.00000000e+00 4.05433867e-03 0.00000000e+00\n",
      "  0.00000000e+00 6.01953859e-02]\n",
      " [2.44899958e-05 0.00000000e+00 0.00000000e+00 4.87759084e-04\n",
      "  0.00000000e+00 2.55716373e-03 9.05844129e-01 0.00000000e+00\n",
      "  3.91288908e-02 0.00000000e+00 1.44082809e-02 0.00000000e+00\n",
      "  1.76327970e-03 3.57860064e-02]\n",
      " [0.00000000e+00 1.76418770e-02 0.00000000e+00 9.31164754e-04\n",
      "  9.15201930e-03 0.00000000e+00 0.00000000e+00 8.69122577e-01\n",
      "  9.91320953e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 4.02026687e-03]\n",
      " [1.16929588e-05 6.82502983e-04 2.62797617e-04 1.22776068e-03\n",
      "  9.43690368e-02 1.21031923e-03 1.25245307e-03 2.19063338e-03\n",
      "  8.91286838e-01 0.00000000e+00 4.76746102e-03 0.00000000e+00\n",
      "  1.40446154e-05 2.72445941e-03]\n",
      " [1.05865618e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.50058509e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 9.34899950e-01 0.00000000e+00 1.83809224e-02\n",
      "  1.36331826e-03 3.12686628e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 2.15940004e-02\n",
      "  1.34857949e-03 1.28809930e-03 9.08489617e-03 0.00000000e+00\n",
      "  9.39141490e-02 0.00000000e+00 8.63713689e-01 0.00000000e+00\n",
      "  0.00000000e+00 9.05658629e-03]\n",
      " [2.40347061e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 9.29854710e-02 0.00000000e+00 8.88798625e-01\n",
      "  1.08372490e-02 7.37625131e-03]\n",
      " [4.10379468e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.12311644e-03 0.00000000e+00\n",
      "  2.79479207e-04 7.46014497e-03 0.00000000e+00 1.17225278e-02\n",
      "  9.18995227e-01 5.63157101e-02]\n",
      " [1.42568286e-01 0.00000000e+00 9.37123917e-04 5.04374246e-04\n",
      "  2.12185442e-01 3.18707910e-03 3.76029014e-03 2.91644972e-04\n",
      "  8.94385062e-03 2.82269444e-02 1.50926273e-03 1.31626238e-03\n",
      "  9.29039347e-03 5.87279047e-01]]\n"
     ]
    }
   ],
   "source": [
    "sae_loss = SAELoss(sigma=0, prior=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wrapper.models\n",
    "import sae.functions.models\n",
    "\n",
    "class SegmentationAutoEncoder(torch.torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        in_channels: int,\n",
    "        out_channels: int, \n",
    "        latent_dim: int\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            - in_channels : nb_channels of image to segmentation\n",
    "            - out_channels : nb_channels of segmented image\n",
    "            - latent_dim : ch\n",
    "        \"\"\"\n",
    "        \n",
    "        super(SegmentationAutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        enc_nf = [4, 8, 16, 32]\n",
    "        dec_nf = [32, 16, 8, 4]\n",
    "        # self.encoder = sae.functions.models.Simple_Unet(\n",
    "        self.encoder = sae.functions.models.Simple_Unet(\n",
    "            input_ch = in_channels,\n",
    "            out_ch = latent_dim,\n",
    "            use_bn = False,\n",
    "            enc_nf = enc_nf,\n",
    "            dec_nf = dec_nf\n",
    "        )\n",
    "\n",
    "        # summary = torch.load(\n",
    "        #     f = './weights/pretrained_encoder.pth.tar',\n",
    "        #     map_location=torch.device('cpu')\n",
    "        # )                        \n",
    "        # _ = self.encoder.load_state_dict(\n",
    "        #     summary['u1']\n",
    "        # ) \n",
    "\n",
    "        # Decoder\n",
    "        # self.decoder = sae.functions.models.Simple_Decoder(\n",
    "        self.decoder = sae.functions.models.Simple_Decoder(\n",
    "            input_ch = latent_dim,\n",
    "            out_ch = out_channels \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "        x: torch.Tensor, \n",
    "        prior, \n",
    "        tau: float, \n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        out = self.encoder(x)\n",
    "        # out = functions.models.enforcer(prior, out)\n",
    "        out = wrapper.models.enforcer(prior, out)\n",
    "        n_batch, chs, dim1, dim2, dim3 = out.size()\n",
    "        logits = out\n",
    "        out = out.permute(0, 2, 3, 4, 1)\n",
    "        out = out.view(n_batch, dim1*dim2*dim3, chs)\n",
    "        # pred = functions.training_tools.gumbel_softmax(out, tau)\n",
    "        pred = wrapper.training_tools.gumbel_softmax(out, tau)\n",
    "        pred = pred.view(n_batch, dim1, dim2, dim3, chs)\n",
    "        pred = pred.permute(0, 4, 1, 2, 3)\n",
    "\n",
    "        recon = self.decoder(pred)\n",
    "\n",
    "        return logits, recon if self.training else recon\n",
    "    \n",
    "    \n",
    "    # To add image in string doc\n",
    "    ## ![Alt text](https://img.freepik.com/vecteurs-libre/vecteur-vintage-pack-ornements-arrondis_23-2147505286.jpg?size=626&ext=jpg \"a title\")\n",
    "    def load_encoder_from(self, pth_file_loaded) -> None:\n",
    "        \"\"\"\n",
    "        You can load a file.pth with:\n",
    "        ```py\n",
    "        torch.load(\n",
    "            f = path/of/your/file.pth,\n",
    "            map_location = torch.device(...)\n",
    "        )\n",
    "        ```\n",
    "\n",
    "        Encoder weights must be trained in first time to map\n",
    "        a template (too called prior). To make a template, you can generate your\n",
    "        image n times and compute number times that a class is\n",
    "        predict.\n",
    "\n",
    "        ..\n",
    "            \\int_{0}^{1} f(x) dx\n",
    "\n",
    "        \"\"\"\n",
    "                      \n",
    "        _ = self.encoder.load_state_dict(pth_file_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_model = SegmentationAutoEncoder(\n",
    "    in_channels = 1, \n",
    "    out_channels = 1, \n",
    "    latent_dim = chs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 160, 192, 224])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, _, _, _ = train_set[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sae_model(x, template, 2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit : torch.Size([1, 14, 160, 192, 224])\n",
      "recon : torch.Size([1, 1, 160, 192, 224])\n"
     ]
    }
   ],
   "source": [
    "logits, recon = out\n",
    "print('logit :', logits.size())\n",
    "print('recon :', recon.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loss = sae_loss(x, logits, recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_path = './data/prob_atlas.npz'\n",
    "template = data.get_prob_atlas(atlas_path)\n",
    "sae_loss = SAELoss(sigma=0, prior=template)\n",
    "# torch.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_step_for_segmentation_auto_encoder_model(\n",
    "    model: SegmentationAutoEncoder,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: SAELoss\n",
    "):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - model : Segmentation Auto Encodeur (SAE)\n",
    "        - optimizer : optimizer\n",
    "        - criterion : SAEloss\n",
    "    \"\"\"  \n",
    "\n",
    "    def train_step(engine, batch) -> None:\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Batch processing\n",
    "        batch_loss = 0\n",
    "        size_of_batch = batch.size()[0]\n",
    "        # predictions = []\n",
    "        acc_logits = []\n",
    "        acc_recons = []\n",
    "        for i in range(0, size_of_batch):\n",
    "            x, _, _, _ = batch[i]\n",
    "            logits, recon = model(x)\n",
    "            loss = criterion(x, logits, recon)\n",
    "            batch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            acc_logits.append(logits)\n",
    "            acc_recons.append(recon)\n",
    "\n",
    "            # predictions.append(prediction.unsqueeze(0))\n",
    "            # loss: torch.Tensor = criterion(prediction, results[i])\n",
    "            # batch_loss += loss.item()\n",
    "            # loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss /= size_of_batch\n",
    "\n",
    "        output = {\n",
    "            'loss' : batch_loss,\n",
    "            'logits' : acc_logits,\n",
    "            'recons' : acc_recons\n",
    "        }\n",
    "\n",
    "        return output\n",
    "    \n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_step_for_unfolding_model(\n",
    "    model: SegmentationAutoEncoder,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: SAELoss\n",
    ") -> tuple:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import functions.dataset\n",
    "\n",
    "\n",
    "def from_sae_config(config: dict) -> tuple:\n",
    "\n",
    "    template = functions.dataset.get_prob_atlas(\n",
    "        path = pathlib.Path(config['dataset']['template'])\n",
    "    )\n",
    "\n",
    "    model = SegmentationAutoEncoder(\n",
    "        in_channels = config['model']['in_channels'],\n",
    "        out_channels = config['model']['out_channels'],\n",
    "        latent_dim = template.shape[1]\n",
    "        # latent_dim = config['model']['latent_dim']\n",
    "\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params = model.parameters(),\n",
    "        lr = config['train']['learning_rate']\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    criterion = SAELoss(\n",
    "       sigma = config['train']['loss']['sigma'],\n",
    "       prior = template,\n",
    "       alpha = config['train']['loss'].get('alpha', 1.0),\n",
    "       beta = config['train']['loss'].get('beta', 0.01),\n",
    "       eps = config['train']['loss'].get('eps', 1e-12),\n",
    "       k = config['train']['loss'].get('eps', 1e-12),\n",
    "       var = config['train']['loss'].get('var', 1e8)\n",
    "    )\n",
    "\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "def from_unfolding_config(config: dict) -> tuple:\n",
    "    pass\n",
    "\n",
    "\n",
    "unfolding_config: dict = {}\n",
    "sae_config : dict = {}\n",
    "\n",
    "unfold_model, unfold_optim, unfold_criterion = \\\n",
    "        from_unfolding_config(unfolding_config)\n",
    "    \n",
    "sae_model, sae_optim, sae_criterion = \\\n",
    "    from_sae_config(sae_config)\n",
    "\n",
    "unfold_train_step = \\\n",
    "    create_train_step_for_unfolding_model(\n",
    "        model=unfold_model,\n",
    "        optimizer=unfold_optim,\n",
    "        criterion=unfold_criterion\n",
    "    )\n",
    "            \n",
    "\n",
    "sae_train_step = \\\n",
    "    create_train_step_for_segmentation_auto_encoder_model(\n",
    "        model=sae_model,\n",
    "        optimizer=sae_optim,\n",
    "        criterion=sae_criterion\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "def create_train_step(\n",
    "   unfolding_train_step: typing.Callable,\n",
    "   sae_train_step: typing.Callable\n",
    ") -> typing.Callable :\n",
    "    \n",
    "    def train_step(engine, batch) -> dict:\n",
    "\n",
    "        res_unfolding = unfolding_train_step(engine, batch)\n",
    "        res_sae = sae_train_step(engine, res_unfolding['predictions'])\n",
    "\n",
    "        output = {\n",
    "            'unfolding' : res_unfolding,\n",
    "            'sae' : res_sae\n",
    "        }\n",
    "\n",
    "        return output\n",
    "    \n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functions import mrf as mrf\n",
    "from functions import models as m\n",
    "from functions import dataset as data\n",
    "from functions import training_tools as tt\n",
    "from functions.visualization import argmax_ch\n",
    "from functions.parser import train_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Loading pretrained weight for enc and dec ============\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# In order obtain good initialization, the encoder was pretrained by\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# mapping the training data to the probabilistic template\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m============ Loading pretrained weight for enc and dec ============\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m summary \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m./weights/pretrained_encoder.pth.tar\u001b[39;49m\u001b[39m'\u001b[39;49m)                        \n\u001b[1;32m     50\u001b[0m u1\u001b[39m.\u001b[39mload_state_dict(summary[\u001b[39m'\u001b[39m\u001b[39mu1\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:1043\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1041\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1042\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1043\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1045\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1047\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:980\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    976\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    978\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m--> 980\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[1;32m    981\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m    982\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    983\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m typed_storage\n\u001b[1;32m    984\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import visualization as vis\n",
    "\n",
    "\n",
    "        \n",
    "\"\"\"Load Data\"\"\"\n",
    "vols_path = './data/vols/'\n",
    "aseg_path = './data/labels/'\n",
    "train_set = data.load_bucker_data(vols_path,\n",
    "                                    aseg_path)\n",
    "\n",
    "\"\"\"Choose template\"\"\"    \n",
    "atlas_path = './data/prob_atlas.npz'\n",
    "template = data.get_prob_atlas(atlas_path)\n",
    "chs = template.shape[1]\n",
    "dim1 = template.shape[2]\n",
    "dim2 = template.shape[3]\n",
    "dim3 = template.shape[4]\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\"\"\"Making Model\"\"\"\n",
    "enc_nf = [4, 8, 16, 32]\n",
    "dec_nf = [32, 16, 8, 4]\n",
    "\n",
    "# Encoder\n",
    "u1 = m.Simple_Unet(input_ch=1,\n",
    "                    out_ch=chs,\n",
    "                    use_bn= False,\n",
    "                    enc_nf= enc_nf,\n",
    "                    dec_nf= dec_nf)\n",
    "# u1 = torch.nn.DataParallel(u1)\n",
    "# u1.cuda()\n",
    "\n",
    "# Decoder\n",
    "u2 = m.Simple_Decoder(chs, 1)\n",
    "# u2 = torch.nn.DataParallel(u2)\n",
    "# u2.cuda()\n",
    "\n",
    "\n",
    "\"\"\"Pretrained Model\"\"\"\n",
    "# In order obtain good initialization, the encoder was pretrained by\n",
    "# mapping the training data to the probabilistic template\n",
    "print('============ Loading pretrained weight for enc and dec ============')\n",
    "summary = torch.load(\n",
    "    './weights/pretrained_encoder.pth.tar',\n",
    "    map_location=torch.device('cpu')\n",
    ")                        \n",
    "u1.load_state_dict(summary['u1']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
