{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On suppose que l'on ait nos images super-résolue.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- Nous appliquons le k-means sur nos images super-résolue\n",
    "    - Nous devons:\n",
    "        - récupérer les clusters\n",
    "        - l'image segmenté comme avant\n",
    "\n",
    "- Nous calculons la probabilité qu'un pixel appartienne à un cluster\n",
    "    - $d(x, C_{i}) = min ~~ \\{ ~~ d(x, y) ~~\\textbf{|}~~ \\forall y \\in C_{i} ~~ \\}$\n",
    "    - $P(x \\notin C_{i}) = \\frac{d(x, C_{i})}{\\sum\\limits_{k=0}^{N} d(x, C_{k})}$\n",
    "    - $P(x \\in C_{i}) = 1 - P(x \\notin C_{i})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_cluster(\n",
    "    pixel_value: float, \n",
    "    cluster: numpy.ndarray\n",
    ") -> numpy.ndarray:\n",
    "    return numpy.min(numpy.abs(pixel_value-cluster))\n",
    "    \n",
    "def probability_clusters(\n",
    "    pixel_value: float, \n",
    "    clusters: numpy.ndarray\n",
    ") -> numpy.ndarray:\n",
    "\n",
    "    distances_clusters = numpy.array(\n",
    "        [\n",
    "            distance_cluster(pixel_value, cluster)\n",
    "            for cluster in clusters\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    total = distances_clusters.sum()\n",
    "\n",
    "    proba_not_in = numpy.array(\n",
    "        [\n",
    "            dist_cluster / total \n",
    "            for dist_cluster in distances_clusters \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    proba_in = 1 - proba_not_in\n",
    "\n",
    "    return proba_in\n",
    "\n",
    "def probability_map(\n",
    "    img: numpy.ndarray, \n",
    "    clusters: numpy.ndarray\n",
    ") -> numpy.ndarray:\n",
    "    \n",
    "    n, m = img.shape\n",
    "    nb_classes = clusters.shape[0]\n",
    "    img_prob = numpy.zeros(shape=(nb_classes, n, m))\n",
    "\n",
    "    for i in range(0, n):\n",
    "        for j in range(0, m):\n",
    "            img_prob[:, i, j] = probability_clusters(\n",
    "                pixel_value = img[i, j],\n",
    "                clusters = clusters \n",
    "            )\n",
    "\n",
    "    return img_prob\n",
    "\n",
    "def likely_probable(\n",
    "    proba_map: numpy.ndarray\n",
    ") -> numpy.ndarray:\n",
    "    \n",
    "    nb_classes, n, m = proba_map.shape\n",
    "    idx_lp = numpy.zeros_like(proba_map)\n",
    "\n",
    "    for i in range(0, n):\n",
    "        for j in range(0, m):\n",
    "            idx_lp[:, i, j] = numpy.where(\n",
    "                proba_map[:, i, j] == proba_map[:, i, j].max()\n",
    "            )\n",
    "\n",
    "    return idx_lp\n",
    "\n",
    "def index_likely_probable(\n",
    "    proba_map: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    _, nb_classes, n, m = proba_map.size()\n",
    "    idx_lp = torch.zeros_like(proba_map, dtype=torch.bool)\n",
    "\n",
    "    for i in range(0, n):\n",
    "        for j in range(0, m):\n",
    "            idx_lp[:, :, i, j] = torch.where(\n",
    "                proba_map[:, :, i, j] == proba_map[:, :, i, j].max(),\n",
    "                True,\n",
    "                False\n",
    "            )\n",
    "\n",
    "    return idx_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template[0, :, 70, 70, 70]\n",
    "# import sys\n",
    "# sys.path.append('./sae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional\n",
    "# import torch.optim\n",
    "\n",
    "# import numpy\n",
    "\n",
    "# import sae.functions.mrf\n",
    "# import sae.functions.visualization\n",
    "# import sae.functions.training_tools\n",
    "\n",
    "# import wrapper.mrf\n",
    "# import wrapper2D.mrf\n",
    "\n",
    "# class SAELoss2D:\n",
    "\n",
    "#     \"\"\"\n",
    "#     Warnings:\n",
    "#         - Running var must be clear to each epoch's end with clear_running_var()\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self,\n",
    "#         sigma: float,\n",
    "#         alpha: float = 1.0, \n",
    "#         beta: float = 0.01, \n",
    "#         eps: float = 1e-12,\n",
    "#         k: int = 3,\n",
    "#         var: float = 1e8\n",
    "#     ) -> None:\n",
    "        \n",
    "#         self.alpha = alpha\n",
    "#         self.beta = beta\n",
    "#         self.sigma = sigma\n",
    "#         self.eps = eps\n",
    "#         self.k = k\n",
    "\n",
    "#         self.var = var\n",
    "\n",
    "#         # self.lookup = None\n",
    "#         # if self.beta != 0:\n",
    "#         #     argm_ch = sae.functions.visualization.argmax_ch(self.prior)\n",
    "#         #     argm_ch = argm_ch.type(torch.uint8)\n",
    "#         #     self.lookup = sae.functions.mrf.get_lookup(\n",
    "#         #         prior = argm_ch,\n",
    "#         #         neighboor_size = self.k\n",
    "#         #     )\n",
    "\n",
    "#         self.running_var = []\n",
    "\n",
    "#     def __call__(self,\n",
    "#         x: torch.Tensor,\n",
    "#         proba_map: torch.Tensor,\n",
    "#         logits: torch.Tensor,\n",
    "#         recon: torch.Tensor\n",
    "#     ) -> torch.Tensor:\n",
    "        \n",
    " \n",
    "#         prior = proba_map # ie template\n",
    "        \n",
    "#         log_prior = torch.log(\n",
    "#             sae.functions.training_tools.normalize_dim1(\n",
    "#                 prior+self.eps\n",
    "#             )\n",
    "#         ).detach()\n",
    "\n",
    "#         lookup = None\n",
    "#         if self.beta != 0:\n",
    "#             argm_ch = index_likely_probable(prior)\n",
    "#             argm_ch = argm_ch.type(torch.uint8)\n",
    "#             # print(argm_ch)\n",
    "#             lookup = wrapper2D.mrf.get_lookup(\n",
    "#                 prior = argm_ch,\n",
    "#                 neighboor_size = self.k\n",
    "#             )\n",
    "        \n",
    "        \n",
    "#         prior_loss = self.compute_prior_loss(logits, log_prior)\n",
    "#         recon_loss = self.compute_recon_loss(x, recon)\n",
    "#         consistent = self.compute_consistent(logits, lookup)\n",
    "\n",
    "#         return prior_loss + recon_loss + consistent\n",
    "    \n",
    "#     def compute_prior_loss(self, \n",
    "#         logits: torch.Tensor,\n",
    "#         log_prior: torch.Tensor\n",
    "#     ) -> torch.Tensor:\n",
    "\n",
    "#         log_pi = torch.nn.functional.log_softmax(logits, 1)\n",
    "#         pi = torch.exp(log_pi)\n",
    "        \n",
    "#         cce = -1*torch.sum(pi*log_prior,1)      #cross entropy\n",
    "#         cce = torch.sum(cce,(1,2))            #cce over all the dims\n",
    "#         cce = cce.mean()               \n",
    "            \n",
    "#         h = -1*torch.sum(pi*log_pi,1)\n",
    "#         h = torch.sum(h,(1,2))\n",
    "#         h = h.mean()\n",
    "\n",
    "#         prior_loss = cce - h\n",
    "\n",
    "#         return prior_loss\n",
    "    \n",
    "#     def compute_consistent(self, \n",
    "#         logits: torch.Tensor,\n",
    "#         lookup: torch.Tensor\n",
    "#     ) -> torch.Tensor:\n",
    "        \n",
    "#         if self.beta != 0: # ie not(self.lookup is None)\n",
    "#             log_pi = torch.nn.functional.log_softmax(logits, 1)\n",
    "#             pi = torch.exp(log_pi)\n",
    "#             consistent = self.beta*wrapper2D.mrf.spatial_consistency(\n",
    "#                 inumpyut = pi,\n",
    "#                 table = lookup,\n",
    "#                 neighboor_size = self.k\n",
    "#             )\n",
    "#         else:\n",
    "#             consistent = torch.zeros(1, device=logits.device)\n",
    "        \n",
    "#         return consistent\n",
    "    \n",
    "#     def compute_recon_loss(self, \n",
    "#         x: torch.Tensor, \n",
    "#         recon: torch.Tensor\n",
    "#     ) -> torch.Tensor:\n",
    "        \n",
    "#         _, _, dim1, dim2 = x.size()\n",
    "        \n",
    "#         if self.sigma == 0:\n",
    "            \n",
    "#             mse = (recon-x.detach())**2  #mse\n",
    "#             mse = torch.sum(mse,(1,2))    #mse over all dims\n",
    "#             mse = mse.mean()                  #avarage over all batches\n",
    "#             recon_loss = self.alpha * mse \n",
    "        \n",
    "#         elif self.sigma == 2:\n",
    "\n",
    "#             # Estimated Variance\n",
    "#             mse = (recon-x.detach())**2\n",
    "#             self.running_var.append(mse.detach().mean().item())\n",
    "\n",
    "#             rounded_var = 10**numpy.round(numpy.log10(self.var))\n",
    "\n",
    "#             # Weight Reconstruction loss\n",
    "#             mse = numpy.clip(0.5*(1/(rounded_var)),0, 500) * mse\n",
    "#             mse = torch.sum(mse,(1,2))    #mse over all dims\n",
    "#             mse = mse.mean()                  #avarage over all batches\n",
    "\n",
    "#             # Since args.var is a scalar now, we need to account for\n",
    "#             # the fact that we doing log det of a matrix\n",
    "#             # Therefore, we multiply by the dimension of the image\n",
    "\n",
    "#             c = dim1*dim2 #chs is 1 for image\n",
    "\n",
    "#             _var = torch.from_numpy(numpy.array(self.var+self.eps)).float()\n",
    "#             recon_loss = mse + 0.5 * c * torch.log(_var)\n",
    "\n",
    "#         else:\n",
    "\n",
    "#             raise AssertionError('sigma must be 0 or 2')\n",
    "        \n",
    "#         return recon_loss\n",
    "    \n",
    "#     def update_variance(self) -> None:\n",
    "#         self.var = numpy.mean(self.running_var)\n",
    "\n",
    "#     def clear_running_var(self) -> None:\n",
    "#         \"\"\" Running var must be clear to each end epoch\n",
    "#         \"\"\"\n",
    "#         self.running_var.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_ROWS, NB_COLS= 512, 512\n",
    "NB_CLASSES = 2 \n",
    "\n",
    "img = torch.zeros(size=(1, 1, NB_ROWS, NB_COLS))\n",
    "prior = torch.zeros(size=(1, NB_CLASSES, NB_ROWS, NB_COLS))\n",
    "prior[0, 0, :, :] = torch.rand(size=(NB_ROWS, NB_COLS))\n",
    "prior[0, 1, :, :] = 1 - prior[0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./sae')\n",
    "import wrapper2D.defineme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wrapper2D.defineme.SegmentationAutoEncoder(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    latent_dim=NB_CLASSES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(img, prior, 2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit : torch.Size([1, 2, 512, 512])\n",
      "recon : torch.Size([1, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "recon, logits = out\n",
    "print('logit :', logits.size())\n",
    "print('recon :', recon.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wrapper2D.defineme\n",
    "sae_loss = wrapper2D.defineme.SAELoss2D(sigma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = sae_loss(\n",
    "    x=img,\n",
    "    proba_map=prior,\n",
    "    logits=logits,\n",
    "    recon=recon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2510721.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_train_step(model: SegmentationAutoEncoder, criterion: SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
