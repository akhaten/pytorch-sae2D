{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Input Libraries'''\n",
    "import os\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functions import mrf as mrf\n",
    "from functions import models as m\n",
    "from functions import dataset as data\n",
    "from functions import training_tools as tt\n",
    "from functions.visualization import argmax_ch\n",
    "from functions.parser import train_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import visualization as vis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/vols/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m vols_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./data/vols/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m aseg_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./data/labels/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m train_set \u001b[39m=\u001b[39m sae\u001b[39m.\u001b[39;49mfunctions\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mload_bucker_data(vols_path,aseg_path)\n",
      "File \u001b[0;32m~/Documents/github/seg-parts/sae/functions/dataset.py:88\u001b[0m, in \u001b[0;36mload_bucker_data.__init__\u001b[0;34m(self, vols_path, aseg_path)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39msuper\u001b[39m(load_bucker_data, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvols_path \u001b[39m=\u001b[39m vols_path\n\u001b[0;32m---> 88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvols_files \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(vols_path)\n\u001b[1;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maseg_path \u001b[39m=\u001b[39m aseg_path\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/vols/'"
     ]
    }
   ],
   "source": [
    "vols_path = './data/vols/'\n",
    "aseg_path = './data/labels/'\n",
    "train_set = data.load_bucker_data(vols_path,aseg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri, aseg, onehot, _  = train_set[0] \n",
    "print('mri :', mri.shape)\n",
    "print('aseg :', aseg.shape)\n",
    "print('onehot :', onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot[0, :, 70, 70, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aseg[70, 70, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 16\n",
    "\n",
    "figure = matplotlib.pyplot.figure()\n",
    "matplotlib.pyplot.subplot(1, 2, 1)\n",
    "_ = matplotlib.pyplot.imshow(mri[0, 0, index])\n",
    "matplotlib.pyplot.subplot(1, 2, 2)\n",
    "_ = matplotlib.pyplot.imshow(aseg[index])\n",
    "figure.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_path = './data/prob_atlas.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = torch.from_numpy(np.load((atlas_path))['vol_data']).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas[70, 70, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = data.get_prob_atlas(atlas_path)\n",
    "print(template.shape)\n",
    "chs = template.shape[1]\n",
    "dim1 = template.shape[2]\n",
    "dim2 = template.shape[3]\n",
    "dim3 = template.shape[4]\n",
    "print(chs, dim1, dim2, dim3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template[0, :, 70, 70, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional\n",
    "import torch.optim\n",
    "\n",
    "import numpy\n",
    "\n",
    "import functions.mrf\n",
    "import functions.visualization\n",
    "import functions.training_tools\n",
    "\n",
    "import wrapper.mrf\n",
    "\n",
    "class SAELoss:\n",
    "\n",
    "    \"\"\"\n",
    "    Warnings:\n",
    "        - Running var must be clear to each epoch's end with clear_running_var()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        sigma: float,\n",
    "        prior,\n",
    "        alpha: float = 1.0, \n",
    "        beta: float = 0.01, \n",
    "        eps: float = 1e-12,\n",
    "        k: int = 3,\n",
    "        var: float = 1e8\n",
    "    ) -> None:\n",
    "        \n",
    "        self.prior = prior\n",
    "        self.log_prior = torch.log(\n",
    "            functions.training_tools.normalize_dim1(prior+eps)\n",
    "        ).detach()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.sigma = sigma\n",
    "        self.eps = eps\n",
    "        self.k = k\n",
    "\n",
    "        self.var = var\n",
    "\n",
    "        self.lookup = None\n",
    "        if self.beta != 0:\n",
    "            argm_ch = functions.visualization.argmax_ch(self.prior)\n",
    "            argm_ch = argm_ch.type(torch.uint8)\n",
    "            self.lookup = functions.mrf.get_lookup(\n",
    "                prior = argm_ch,\n",
    "                neighboor_size = self.k\n",
    "            )\n",
    "\n",
    "        self.running_var = []\n",
    "\n",
    "\n",
    "    def __call__(self,\n",
    "        x: torch.Tensor,\n",
    "        logits: torch.Tensor,\n",
    "        recon: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        prior_loss = self.compute_prior_loss(logits)\n",
    "        recon_loss = self.compute_recon_loss(x, recon)\n",
    "        consistent = self.compute_consistent(logits)\n",
    "        return prior_loss + recon_loss + consistent\n",
    "\n",
    "    def compute_prior_loss(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        log_pi = F.log_softmax(logits, 1)\n",
    "        pi = torch.exp(log_pi)\n",
    "        \n",
    "        cce = -1*torch.sum(pi*self.log_prior,1)      #cross entropy\n",
    "        cce = torch.sum(cce,(1,2,3))            #cce over all the dims\n",
    "        cce = cce.mean()               \n",
    "            \n",
    "        h = -1*torch.sum(pi*log_pi,1)\n",
    "        h = torch.sum(h,(1,2,3))\n",
    "        h = h.mean()\n",
    " \n",
    "        prior_loss = cce - h\n",
    "\n",
    "        return prior_loss\n",
    "    \n",
    "    def compute_consistent(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        log_pi = F.log_softmax(logits, 1)\n",
    "        pi = torch.exp(log_pi)\n",
    "        \n",
    "        if self.beta != 0: # ie not(self.lookup is None)\n",
    "            consistent = self.beta*wrapper.mrf.spatial_consistency(\n",
    "                input = pi,\n",
    "                table = self.lookup,\n",
    "                neighboor_size = self.k\n",
    "            )\n",
    "        else:\n",
    "            consistent = torch.zeros(1, device=logits.device)\n",
    "        \n",
    "        return consistent\n",
    "    \n",
    "    def compute_recon_loss(self, \n",
    "        x: torch.Tensor, \n",
    "        recon: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        if self.sigma == 0:\n",
    "            mse = (recon-x.detach())**2  #mse\n",
    "            mse = torch.sum(mse,(1,2,3,4))    #mse over all dims\n",
    "            mse = mse.mean()                  #avarage over all batches\n",
    "            recon_loss = self.alpha * mse \n",
    "        elif self.sigma == 2:\n",
    "            mse = (recon-x.detach())**2\n",
    "            rounded_var = 10**np.round(np.log10(self.var))\n",
    "\n",
    "            # Weight Reconstruction loss\n",
    "            mse = np.clip(0.5*(1/(rounded_var)),0, 500) * mse\n",
    "            mse = torch.sum(mse,(1,2,3,4))    #mse over all dims\n",
    "            mse = mse.mean()                  #avarage over all batches\n",
    "\n",
    "            self.running_var.append(mse.detach().mean().item())\n",
    "\n",
    "            # Since args.var is a scalar now, we need to account for\n",
    "            # the fact that we doing log det of a matrix\n",
    "            # Therefore, we multiply by the dimension of the image\n",
    "\n",
    "            c = dim1*dim2*dim3 #chs is 1 for image\n",
    "\n",
    "            _var = torch.from_numpy(np.array(self.var+self.eps)).float()\n",
    "            recon_loss = mse + 0.5 * c * torch.log(_var)\n",
    "        else:\n",
    "            raise AssertionError('sigma must be 0 or 2')\n",
    "        \n",
    "        return recon_loss\n",
    "    \n",
    "    def update_variance(self) -> None:\n",
    "        self.var = numpy.mean(self.running_var)\n",
    "\n",
    "    def clear_running_var(self) -> None:\n",
    "        \"\"\" Running var must be clear to each end epoch\n",
    "        \"\"\"\n",
    "        self.running_var.clear()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argm_ch = functions.visualization.argmax_ch(template).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = functions.mrf.get_lookup(\n",
    "#     prior = argm_ch,\n",
    "#     neighboor_size = 3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior = template.type(dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior.dtype, template.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert prior.dtype == torch.uint8, 'The prior should be one-hot encoded byte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_loss = SAELoss(sigma=0, prior=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(template[0, :, 100, 100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = len(train_set)\n",
    "# train_idx = np.arange(indices)\n",
    "# train_sampler = tt.Sampler(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = len(train_set)\n",
    "# \"\"\"Making Model\"\"\"\n",
    "# enc_nf = [4, 8, 16, 32]\n",
    "# dec_nf = [32, 16, 8, 4]\n",
    "\n",
    "# # Encoder\n",
    "# u1 = m.Simple_Unet(input_ch=1,\n",
    "#                     out_ch=chs,\n",
    "#                     use_bn= False,\n",
    "#                     enc_nf= enc_nf,\n",
    "#                     dec_nf= dec_nf)\n",
    "# u1 = torch.nn.DataParallel(u1)\n",
    "# # u1.cuda()\n",
    "\n",
    "# # Decoder\n",
    "# u2 = m.Simple_Decoder(chs, 1)\n",
    "# u2 = torch.nn.DataParallel(u2)\n",
    "# # u2.cuda()\n",
    "\n",
    "# params = list(u1.parameters()) + list(u2.parameters())\n",
    "\n",
    "# # optimizer = torch.optim.Adam(params, lr= args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "import functions.models\n",
    "import functions.training_tools\n",
    "\n",
    "import wrapper.training_tools\n",
    "import wrapper.models\n",
    "\n",
    "class SegmentationAutoEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        in_channels: int,\n",
    "        out_channels: int, \n",
    "        latent_dim: int\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            - in_channels : nb_channels of image to segmentation\n",
    "            - out_channels : nb_channels of segmented image\n",
    "            - latent_dim : ch\n",
    "        \"\"\"\n",
    "        \n",
    "        super(SegmentationAutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        enc_nf = [4, 8, 16, 32]\n",
    "        dec_nf = [32, 16, 8, 4]\n",
    "        self.encoder = functions.models.Simple_Unet(\n",
    "            input_ch = in_channels,\n",
    "            out_ch = latent_dim,\n",
    "            use_bn = False,\n",
    "            enc_nf = enc_nf,\n",
    "            dec_nf = dec_nf\n",
    "        )\n",
    "        summary = torch.load(\n",
    "            f = './weights/pretrained_encoder.pth.tar',\n",
    "            map_location=torch.device('cpu')\n",
    "        )                        \n",
    "        _ = self.encoder.load_state_dict(\n",
    "            summary['u1']\n",
    "        ) \n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = functions.models.Simple_Decoder(\n",
    "            input_ch = latent_dim,\n",
    "            out_ch = out_channels \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "        x: torch.Tensor, \n",
    "        prior, \n",
    "        tau: float, \n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        out = self.encoder(x)\n",
    "        # out = functions.models.enforcer(prior, out)\n",
    "        out = wrapper.models.enforcer(prior, out)\n",
    "        n_batch, chs, dim1, dim2, dim3 = out.size()\n",
    "        logits = out\n",
    "        out = out.permute(0, 2, 3, 4, 1)\n",
    "        out = out.view(n_batch, dim1*dim2*dim3, chs)\n",
    "        # pred = functions.training_tools.gumbel_softmax(out, tau)\n",
    "        pred = wrapper.training_tools.gumbel_softmax(out, tau)\n",
    "        pred = pred.view(n_batch, dim1, dim2, dim3, chs)\n",
    "        pred = pred.permute(0, 4, 1, 2, 3)\n",
    "\n",
    "        recon = self.decoder(pred)\n",
    "\n",
    "        return logits, recon if self.training else recon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_model = SegmentationAutoEncoder(\n",
    "    in_channels = 1, \n",
    "    out_channels = 1, \n",
    "    latent_dim = chs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _, _, _ = train_set[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sae_model(x, template, 2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, recon = out\n",
    "print('logit :', logits.size())\n",
    "print('recon :', recon.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loss = sae_loss(x, logits, recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_path = './data/prob_atlas.npz'\n",
    "template = data.get_prob_atlas(atlas_path)\n",
    "sae_loss = SAELoss(sigma=0, prior=template)\n",
    "# torch.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_step_for_segmentation_auto_encoder_model(\n",
    "    model: SegmentationAutoEncoder,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: SAELoss\n",
    "):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - model : Segmentation Auto Encodeur (SAE)\n",
    "        - optimizer : optimizer\n",
    "        - criterion : SAEloss\n",
    "    \"\"\"  \n",
    "\n",
    "    def train_step(engine, batch) -> None:\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Batch processing\n",
    "        batch_loss = 0\n",
    "        size_of_batch = batch.size()[0]\n",
    "        # predictions = []\n",
    "        acc_logits = []\n",
    "        acc_recons = []\n",
    "        for i in range(0, size_of_batch):\n",
    "            x, _, _, _ = batch[i]\n",
    "            logits, recon = model(x)\n",
    "            loss = criterion(x, logits, recon)\n",
    "            batch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            acc_logits.append(logits)\n",
    "            acc_recons.append(recon)\n",
    "\n",
    "            # predictions.append(prediction.unsqueeze(0))\n",
    "            # loss: torch.Tensor = criterion(prediction, results[i])\n",
    "            # batch_loss += loss.item()\n",
    "            # loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss /= size_of_batch\n",
    "\n",
    "        output = {\n",
    "            'loss' : batch_loss,\n",
    "            'logits' : acc_logits,\n",
    "            'recons' : acc_recons\n",
    "        }\n",
    "\n",
    "        return output\n",
    "    \n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_step_for_unfolding_model(\n",
    "    model: SegmentationAutoEncoder,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: SAELoss\n",
    ") -> tuple:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import functions.dataset\n",
    "\n",
    "\n",
    "def from_sae_config(config: dict) -> tuple:\n",
    "\n",
    "    template = functions.dataset.get_prob_atlas(\n",
    "        path = pathlib.Path(config['dataset']['template'])\n",
    "    )\n",
    "\n",
    "    model = SegmentationAutoEncoder(\n",
    "        in_channels = config['model']['in_channels'],\n",
    "        out_channels = config['model']['out_channels'],\n",
    "        latent_dim = template.shape[1]\n",
    "        # latent_dim = config['model']['latent_dim']\n",
    "\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params = model.parameters(),\n",
    "        lr = config['train']['learning_rate']\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    criterion = SAELoss(\n",
    "       sigma = config['train']['loss']['sigma'],\n",
    "       prior = template,\n",
    "       alpha = config['train']['loss'].get('alpha', 1.0),\n",
    "       beta = config['train']['loss'].get('beta', 0.01),\n",
    "       eps = config['train']['loss'].get('eps', 1e-12),\n",
    "       k = config['train']['loss'].get('eps', 1e-12),\n",
    "       var = config['train']['loss'].get('var', 1e8)\n",
    "    )\n",
    "\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "def from_unfolding_config(config: dict) -> tuple:\n",
    "    pass\n",
    "\n",
    "\n",
    "unfolding_config: dict = {}\n",
    "sae_config : dict = {}\n",
    "\n",
    "unfold_model, unfold_optim, unfold_criterion = \\\n",
    "        from_unfolding_config(unfolding_config)\n",
    "    \n",
    "sae_model, sae_optim, sae_criterion = \\\n",
    "    from_sae_config(sae_config)\n",
    "\n",
    "unfold_train_step = \\\n",
    "    create_train_step_for_unfolding_model(\n",
    "        model=unfold_model,\n",
    "        optimizer=unfold_optim,\n",
    "        criterion=unfold_criterion\n",
    "    )\n",
    "            \n",
    "\n",
    "sae_train_step = \\\n",
    "    create_train_step_for_segmentation_auto_encoder_model(\n",
    "        model=sae_model,\n",
    "        optimizer=sae_optim,\n",
    "        criterion=sae_criterion\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "def create_train_step(\n",
    "   unfolding_train_step: typing.Callable,\n",
    "   sae_train_step: typing.Callable\n",
    ") -> typing.Callable :\n",
    "    \n",
    "    def train_step(engine, batch) -> dict:\n",
    "\n",
    "        res_unfolding = unfolding_train_step(engine, batch)\n",
    "        res_sae = sae_train_step(engine, res_unfolding['predictions'])\n",
    "\n",
    "        output = {\n",
    "            'unfolding' : res_unfolding,\n",
    "            'sae' : res_sae\n",
    "        }\n",
    "\n",
    "        return output\n",
    "    \n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functions import mrf as mrf\n",
    "from functions import models as m\n",
    "from functions import dataset as data\n",
    "from functions import training_tools as tt\n",
    "from functions.visualization import argmax_ch\n",
    "from functions.parser import train_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Loading pretrained weight for enc and dec ============\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# In order obtain good initialization, the encoder was pretrained by\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# mapping the training data to the probabilistic template\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m============ Loading pretrained weight for enc and dec ============\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m summary \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m./weights/pretrained_encoder.pth.tar\u001b[39;49m\u001b[39m'\u001b[39;49m)                        \n\u001b[1;32m     50\u001b[0m u1\u001b[39m.\u001b[39mload_state_dict(summary[\u001b[39m'\u001b[39m\u001b[39mu1\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:1043\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1041\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1042\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1043\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1045\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1047\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:980\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    976\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    978\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m--> 980\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[1;32m    981\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m    982\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    983\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m typed_storage\n\u001b[1;32m    984\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m/nix/store/f4w6jmdhyw2m1wzb86mh2qk0gqrrpswp-python3.10-torch-2.0.1/lib/python3.10/site-packages/torch/serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import visualization as vis\n",
    "\n",
    "\n",
    "        \n",
    "\"\"\"Load Data\"\"\"\n",
    "vols_path = './data/vols/'\n",
    "aseg_path = './data/labels/'\n",
    "train_set = data.load_bucker_data(vols_path,\n",
    "                                    aseg_path)\n",
    "\n",
    "\"\"\"Choose template\"\"\"    \n",
    "atlas_path = './data/prob_atlas.npz'\n",
    "template = data.get_prob_atlas(atlas_path)\n",
    "chs = template.shape[1]\n",
    "dim1 = template.shape[2]\n",
    "dim2 = template.shape[3]\n",
    "dim3 = template.shape[4]\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\"\"\"Making Model\"\"\"\n",
    "enc_nf = [4, 8, 16, 32]\n",
    "dec_nf = [32, 16, 8, 4]\n",
    "\n",
    "# Encoder\n",
    "u1 = m.Simple_Unet(input_ch=1,\n",
    "                    out_ch=chs,\n",
    "                    use_bn= False,\n",
    "                    enc_nf= enc_nf,\n",
    "                    dec_nf= dec_nf)\n",
    "# u1 = torch.nn.DataParallel(u1)\n",
    "# u1.cuda()\n",
    "\n",
    "# Decoder\n",
    "u2 = m.Simple_Decoder(chs, 1)\n",
    "# u2 = torch.nn.DataParallel(u2)\n",
    "# u2.cuda()\n",
    "\n",
    "\n",
    "\"\"\"Pretrained Model\"\"\"\n",
    "# In order obtain good initialization, the encoder was pretrained by\n",
    "# mapping the training data to the probabilistic template\n",
    "print('============ Loading pretrained weight for enc and dec ============')\n",
    "summary = torch.load(\n",
    "    './weights/pretrained_encoder.pth.tar',\n",
    "    map_location=torch.device('cpu')\n",
    ")                        \n",
    "u1.load_state_dict(summary['u1']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
